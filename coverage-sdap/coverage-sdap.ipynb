{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td><img src=\"logos/JPL-NASA-logo_583x110.png\" alt=\"JPL/NASA logo\" style=\"height: 75px\"/></td>\n",
    "    <td><img src=\"logos/CEOS-LOGO.png\" alt=\"CEOS logo\" style=\"height: 75px\"/></td>\n",
    "    <td><img src=\"logos/CoverageLogoFullClear.png\" alt=\"COVERAGE logo\" style=\"height: 100px\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Analytics Examples for COVERAGE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Notes \n",
    "When you first connected you should have seen two notebook folders, `coverage` and `work`.  The original version of this notebook is in the `coverage` folder and is read-only.  If you would like to modify the code samples in this notebook, please first click `File`->`Save as...` to save your own copy in the `work` folder instead, and make your modifications to that copy. \n",
    "\n",
    "We don't yet have resources in place to support a true multi-user environment for notebooks.  This means that all saved notebooks are visible to all users. Thus, it would help to include your own unique identifier in the notebook name to avoid conflicts with others.  \n",
    "\n",
    "Furthermore, we do not guarantee to preserve the saved notebooks for any period of time.  If you would like to keep your notebook, please remember to click `File`->`Download as`->`Notebook (.ipynb)` to download your own copy of the notebook at the end of each editting session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup\n",
    "\n",
    "In the cell below are a few functions that help with plotting data using matplotlib. You shouldn't need to modify or pay much attention to this cell. Just run the cell to define the functions so that they can be used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#######################################################################################\n",
    "# In some jupyter deployments you will get an error about PROJ_LIB not being defined.\n",
    "# In that case, uncomment these lines and set the directory to the location of your\n",
    "# proj folder.\n",
    "# import os\n",
    "# import sys\n",
    "# # Find where we are on the computer and make sure it is the pyICM directory\n",
    "# HomeDir = os.path.expanduser('~')  # get the home directory\n",
    "\n",
    "# ICMDir = HomeDir + \"Desktop/AIST_Project/SDAP_Jupyter\"\n",
    "# # Navigate to the home directory\n",
    "# os.chdir(ICMDir)\n",
    "# print('Moved to Directory',ICMDir)\n",
    "\n",
    "# module_path = os.path.join(ICMDir,'code')\n",
    "# print('Code Directory is',module_path)\n",
    "# print('Adding to the system path')\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "#######################################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as mticker\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "import cartopy.crs as ccrs #added by B Clark because Basemap is deprecated\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import numpy as np\n",
    "import types\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from shapely.geometry import box\n",
    "from pprint import pprint, pformat\n",
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ISO-8601 date format\n",
    "dt_format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "\n",
    "def show_plot(x_data, y_data, x_label=None, y_label=None, title=None):\n",
    "    \"\"\"\n",
    "    Display a simple line plot.\n",
    "    \n",
    "    :param x_data: Numpy array containing data for the X axis\n",
    "    :param y_data: Numpy array containing data for the Y axis\n",
    "    :param x_label: Label applied to X axis\n",
    "    :param y_label: Label applied to Y axis\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,3), dpi=100)\n",
    "    plt.plot([datetime.fromtimestamp(x_val) for x_val in x_data], y_data, 'b-', marker='|', markersize=2.0, mfc='b')\n",
    "    plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if x_label is not None:\n",
    "        plt.xlabel(x_label)\n",
    "    if y_label is not None:\n",
    "        plt.ylabel (y_label)\n",
    "    plt.xticks(rotation=45)\n",
    "    ts_range = x_data[-1] - x_data[0]\n",
    "\n",
    "    # Define the time formatting\n",
    "    if ts_range > 189216000: # 6 years\n",
    "        dtFmt = mdates.DateFormatter('%Y')\n",
    "    elif ts_range > 15552000: # 6 months\n",
    "        dtFmt = mdates.DateFormatter('%b %Y')\n",
    "    else: # < 6 months\n",
    "        dtFmt = mdates.DateFormatter('%b %-d, %Y')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(dtFmt)\n",
    "    plt.show()\n",
    "\n",
    "def plot_box(bbox):\n",
    "    \"\"\"\n",
    "    Display a Green bounding box on an image of the blue marble.\n",
    "    \n",
    "    :param bbox: Shapely Polygon that defines the bounding box to display\n",
    "    \"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox.bounds\n",
    "    import matplotlib.pyplot as plt1\n",
    "    import cartopy.crs as ccrs #added by B Clark because Basemap is deprecated\n",
    "    # modified 11/30/2021 to use Cartopy toolbox B Clark NASA GSFC\n",
    "    # from matplotlib.patches import Polygon\n",
    "    # from mpl_toolkits.basemap import Basemap\n",
    "    from shapely.geometry.polygon import Polygon\n",
    "    # map = Basemap()\n",
    "    # map.bluemarble(scale=0.5)\n",
    "    # poly = Polygon([(min_lon,min_lat),(min_lon,max_lat),(max_lon,max_lat),(max_lon,min_lat)],\n",
    "    #                facecolor=(0,0,0,0.0),edgecolor='green',linewidth=2)\n",
    "    # plt1.gca().add_patch(poly)\n",
    "    # plt1.gcf().set_size_inches(10,15)\n",
    "    ax = plt1.axes(projection=ccrs.PlateCarree())\n",
    "    ax.stock_img()\n",
    "    # plt.show()     \n",
    "    poly = Polygon(((min_lon,min_lat),(min_lon,max_lat),(max_lon,max_lat),(max_lon,min_lat),(min_lon,min_lat)))\n",
    "    ax.add_geometries([poly],crs=ccrs.PlateCarree(),facecolor='b', edgecolor='red', alpha=0.8)\n",
    "    # ax.fill(x, y,  color='coral', alpha=0.4)\n",
    "    # plt1.gca().add_patch(poly)\n",
    "    # plt1.gcf().set_size_inches(10,15)\n",
    "    plt1.show()\n",
    "\n",
    "def show_plot_two_series(x_data_a, x_data_b, y_data_a, y_data_b, x_label, \n",
    "                         y_label_a, y_label_b, series_a_label, series_b_label,\n",
    "                         title=''):\n",
    "    \"\"\"\n",
    "    Display a line plot of two series\n",
    "    \n",
    "    :param x_data_a: Numpy array containing data for the Series A X axis\n",
    "    :param x_data_b: Numpy array containing data for the Series B X axis\n",
    "    :param y_data_a: Numpy array containing data for the Series A Y axis\n",
    "    :param y_data_b: Numpy array containing data for the Series B Y axis\n",
    "    :param x_label: Label applied to X axis\n",
    "    :param y_label_a: Label applied to Y axis for Series A\n",
    "    :param y_label_b: Label applied to Y axis for Series B\n",
    "    :param series_a_label: Name of Series A\n",
    "    :param series_b_label: Name of Series B\n",
    "    \"\"\" \n",
    "    font_size=12\n",
    "    plt.rc('font', size=font_size)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=font_size)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=font_size)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=font_size)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=font_size)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=font_size)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=font_size)  # fontsize of the figure title\n",
    "    fig, ax1 = plt.subplots(figsize=(10,5), dpi=100)\n",
    "    series_a, = ax1.plot(x_data_a, y_data_a, 'b-', marker='|', markersize=2.0, mfc='b', label=series_a_label)\n",
    "    ax1.set_ylabel(y_label_a, color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    ax1.set_ylim(min(0, *y_data_a), max(y_data_a)+.1*max(y_data_a))\n",
    "    ax1.set_xlabel(x_label)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    series_b, = ax2.plot(x_data_b, y_data_b, 'r-', marker='|', markersize=2.0, mfc='r', label=series_b_label)\n",
    "    ax2.set_ylabel(y_label_b, color='r')\n",
    "    ax2.set_ylim(min(0, *y_data_b), max(y_data_b)+.1*max(y_data_b))\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    \n",
    "    plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "    plt.legend(handles=(series_a, series_b), bbox_to_anchor=(1.1, 1), loc=2, borderaxespad=0.)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def ts_plot_two(ts_json1, ts_json2, dataset1, dataset2, units1, units2,\n",
    "            title='', t_name='time', val_name='mean'):\n",
    "    t1 = np.array([ts[0][t_name] for ts in ts_json1[\"data\"]])\n",
    "    t2 = np.array([ts[0][t_name] for ts in ts_json2[\"data\"]])\n",
    "    vals1 = np.array([ts[0][val_name] for ts in ts_json1[\"data\"]])\n",
    "    vals2 = np.array([ts[0][val_name] for ts in ts_json2[\"data\"]])\n",
    "    show_plot_two_series(t1, t2, vals1, vals2, \"time (sec since 1970-01-01T00:00:00)\",\n",
    "                         units1, units2, dataset1, dataset2, title=title)\n",
    "\n",
    "def scatter_plot(ts_json1, ts_json2, t_name=\"time\", val_name=\"mean\",\n",
    "                title=\"\", xlabel=\"\", ylabel=\"\"):\n",
    "    times1 = np.array([ts[0][t_name] for ts in ts_json1[\"data\"]])\n",
    "    times2 = np.array([ts[0][t_name] for ts in ts_json2[\"data\"]])\n",
    "    vals1 = np.array([ts[0][val_name] for ts in ts_json1[\"data\"]])\n",
    "    vals2 = np.array([ts[0][val_name] for ts in ts_json2[\"data\"]])\n",
    "    vals_x = []\n",
    "    vals_y = []\n",
    "    for i1,t1 in enumerate(times1):\n",
    "        i = (np.abs(times2-times1[i1])).argmin()\n",
    "        if np.abs(times1[i1]-times2[i]) < 86400: # 24 hrs\n",
    "            vals_x.append(vals1[i1])\n",
    "            vals_y.append(vals2[i])\n",
    "    plt.scatter(vals_x, vals_y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "def roundBorders(borders, borderSlop=10.):\n",
    "    b0 = roundBorder(borders[0], 'down', borderSlop,   0.)\n",
    "    b1 = roundBorder(borders[1], 'down', borderSlop, -90.)\n",
    "    b2 = roundBorder(borders[2],   'up', borderSlop, 360.)\n",
    "    b3 = roundBorder(borders[3],   'up', borderSlop,  90.)\n",
    "    return [b0, b1, b2, b3]\n",
    "\n",
    "def roundBorder(val, direction, step, end):\n",
    "    if direction == 'up':\n",
    "        rounder = math.ceil\n",
    "        slop = step\n",
    "    else:\n",
    "        rounder = math.floor\n",
    "        slop = -step\n",
    "###    v = rounder(val/step) * step + slop                                      \n",
    "    v = rounder(val/step) * step\n",
    "    if abs(v - end) < step+1.: v = end\n",
    "    return v\n",
    "\n",
    "def normalizeLon(lon):\n",
    "    if lon < 0.: return lon + 360.\n",
    "    if lon > 360.: return lon - 360.\n",
    "    return lon\n",
    "\n",
    "def normalizeLons(lons):\n",
    "    return np.array([normalizeLon(lon) for lon in lons])\n",
    "\n",
    "def ensureItems(d1, d2):\n",
    "    for key in d2.keys():\n",
    "        if key not in d1: d1[key] = d2[key]\n",
    "\n",
    "CmdOptions = {'MCommand':  ['title', 'xlabel', 'ylabel',  'xlim', 'ylim', 'show\\\n",
    "'],\n",
    "   \t      'plot':      ['label', 'linewidth', 'legend', 'axis'],\n",
    "              'map.plot':  ['label', 'linewidth', 'axis'],\n",
    "              'map.scatter':  ['norm', 'alpha', 'linewidths', 'faceted', 'hold'\\\n",
    "],\n",
    "              'savefig':   ['dpi', 'orientation']\n",
    "              }\n",
    "\n",
    "def die(*s):   warn('Error,',  *s); sys.exit()\n",
    "    \n",
    "def evalKeywordCmds(options, cmdOptions=CmdOptions):\n",
    "    for option in options:\n",
    "        if option in cmdOptions['MCommand']:\n",
    "            args = options[option]\n",
    "            if args:\n",
    "                if args is True:\n",
    "                    args = ''\n",
    "                else:\n",
    "                    args = \"'\" + args + \"'\"\n",
    "                if option in cmdOptions:\n",
    "                    args += dict2kwargs( validCmdOptions(options, cmdOptions[option]) )\n",
    "                try:\n",
    "                    eval('plt.' + option + '(%s)' % args)\n",
    "                except:\n",
    "                    die('failed eval of keyword command option failed: %s=%s' % (option, args))\n",
    "\n",
    "def validCmdOptions(options, cmd, possibleOptions=CmdOptions):\n",
    "    return dict([(option, options[option]) for option in options.keys()\n",
    "                    if option in possibleOptions[cmd]])\n",
    "\n",
    "def dict2kwargs(d):\n",
    "    args = [',%s=%s' % (kw, d[kw]) for kw in d]\n",
    "    return ', '.join(args)\n",
    "\n",
    "def imageMap(lons, lats, vals, vmin=None, vmax=None, \n",
    "             imageWidth=None, imageHeight=None, outFile=None,\n",
    "             projection='cyl', cmap=plt.cm.jet, logColors=False, makeFigure=False,\n",
    "             borders=[0., -90., 360., 90.], autoBorders=True, borderSlop=10.,\n",
    "             meridians=[0, 360, 60], parallels=[-60, 90, 30], title='', normalizeLongs=True,\n",
    "             **options):\n",
    "    if normalizeLongs:\n",
    "        lons = normalizeLons(lons)\n",
    "    if vmin == 'auto': vmin = None\n",
    "    if vmax == 'auto': vmax = None\n",
    "    if imageWidth is not None: makeFigure = True\n",
    "    if projection is None or projection == '': projection = 'cyl'\n",
    "    if cmap is None or cmap == '': cmap = plt.cm.jet\n",
    "    #if isinstance(cmap, types.StringType) and cmap != '':\n",
    "    if isinstance(cmap, str) and cmap != '':\n",
    "        try:\n",
    "            cmap = eval('plt.cm.' + cmap)\n",
    "        except:\n",
    "            cmap = plt.cm.jet\n",
    "\n",
    "    ensureItems(options, { \\\n",
    "                     'title': title, 'dpi': 100,\n",
    "                     'imageWidth': imageWidth or 1024, 'imageHeight': imageHeight or 768})\n",
    "    if autoBorders:\n",
    "        borders = [min(lons), min(lats), max(lons), max(lats)]\n",
    "        borders = roundBorders(borders, borderSlop)\n",
    "\n",
    "    #m = Basemap(borders[0], borders[1], borders[2], borders[3], \\\n",
    "    #            projection=projection, lon_0=np.average([lons[0], lons[-1]]))\n",
    "\n",
    "    if makeFigure:\n",
    "        dpi = float(options['dpi'])\n",
    "        width = float(imageWidth) / dpi\n",
    "        height = width\n",
    "        #if imageHeight is None:\n",
    "        #    height = width * m.aspect\n",
    "        #else:\n",
    "        #    height = float(imageHeight) / dpi\n",
    "        #plt.figure(figsize=(width,height)).add_axes([0.1,0.1,0.8,0.8], frameon=True)\n",
    "        plt.figure(figsize=(width,height))\n",
    "        m = plt.axes(projection=ccrs.PlateCarree())\n",
    "        #m.set_extent([meridians[0], meridians[1], parallels[0], parallels[1]],\n",
    "        #             crs=ccrs.PlateCarree())\n",
    "        gl = m.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                         linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlabels_top = False\n",
    "        gl.ylabels_left = True\n",
    "        gl.ylabels_right = False\n",
    "        gl.xlines = True\n",
    "        gl.ylines = True\n",
    "        gl.xlocator = mticker.FixedLocator(np.arange(meridians[0], meridians[1]+meridians[2], meridians[2]))\n",
    "        gl.ylocator = mticker.FixedLocator(np.arange(parallels[0], parallels[1]+parallels[2], parallels[2]))\n",
    "        gl.xformatter = LONGITUDE_FORMATTER\n",
    "        gl.yformatter = LATITUDE_FORMATTER\n",
    "        gl.xlabel_style = {'size': 12, 'color': 'black'}\n",
    "        gl.ylabel_style = {'size': 12, 'color': 'black'}\n",
    "        \n",
    "    if vmin is not None or vmax is not None: \n",
    "        if vmin is None:\n",
    "            vmin = np.min(vals)\n",
    "        else:\n",
    "            vmin = float(vmin)\n",
    "        if vmax is None:\n",
    "            vmax = np.max(vals)\n",
    "        else:\n",
    "            vmax = float(vmax)\n",
    "        #vrange = (vmax - vmin) / 255.\n",
    "        #levels = np.arange(vmin, vmax, vrange/30.)\n",
    "        levels = np.linspace(vmin, vmax, 256)\n",
    "    else:\n",
    "        levels = 30\n",
    "\n",
    "    if logColors:\n",
    "        norm = mcolors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "    else:\n",
    "        norm = None\n",
    "    # x, y = m(*np.meshgrid(lons,lats))\n",
    "    x, y = np.meshgrid(lons,lats)\n",
    "    c = m.contourf(x, y, vals, levels, cmap=cmap, colors=None, norm=norm)\n",
    "    # m.drawcoastlines()\n",
    "    m.coastlines()\n",
    "    #m.drawmeridians(range(meridians[0], meridians[1], meridians[2]), labels=[0,0,0,1])\n",
    "    #m.drawparallels(range(parallels[0], parallels[1], parallels[2]), labels=[1,1,1,1])\n",
    "    plt.colorbar(c, ticks=np.linspace(vmin,vmax,7), shrink=0.6)\n",
    "    evalKeywordCmds(options)\n",
    "    if outFile:\n",
    "        plt.savefig(outFile, **validCmdOptions(options, 'savefig'))\n",
    "    \n",
    "def arr2d_from_json(js, var_name):\n",
    "    return np.array([[js[i][j][var_name] for j in range(len(js[0]))] for i in range(len(js))])\n",
    "\n",
    "def arr1d_from_json(js, var_name):\n",
    "    return np.array([js[i][var_name] for i in range(len(js))])\n",
    "\n",
    "def plot_map(map, val_key=\"mean\", cnt_key=\"cnt\", lon_key=\"lon\", lat_key=\"lat\", fill=-9999, grid_line_sep=10,\n",
    "             border_slop=1, log_colors=False, title='', vmin=None, vmax=None, \n",
    "             normalize_lons=False, image_width=1000, **options):\n",
    "    # Parse values, longitudes and latitudes from JSON response.\n",
    "    vals = arr2d_from_json(map, val_key)\n",
    "    cnts = arr2d_from_json(map, cnt_key)\n",
    "    lons = arr1d_from_json(map[0], lon_key)\n",
    "    lats = arr1d_from_json([map[i][0] for i in range(len(map))], lat_key)\n",
    "    \n",
    "    # If cnt is 0, set value to fill\n",
    "    vals[cnts==0] = fill\n",
    "    \n",
    "    # Plot time time-averaged map as an image.\n",
    "    print(\"Creating plot of the results.\")\n",
    "    print(\"This will take a minute.  Please wait...\")\n",
    "    min_val = np.min(vals[vals != fill])\n",
    "    if vmin is None:\n",
    "        vmin = min_val\n",
    "    max_val = np.max(vals[vals != fill])\n",
    "    if vmax is None:\n",
    "        vmax = max_val\n",
    "    min_lon = math.floor(np.min(lons)) - grid_line_sep\n",
    "    max_lon = math.ceil(np.max(lons))\n",
    "    min_lat = math.floor(np.min(lats)) - grid_line_sep\n",
    "    max_lat = math.ceil(np.max(lats))\n",
    "    imageMap(lons, lats, vals, imageWidth=image_width, vmin=vmin, vmax=vmax, logColors=log_colors,\n",
    "             meridians=[min_lon, max_lon, grid_line_sep], \n",
    "             parallels=[min_lat, max_lat, grid_line_sep], borderSlop=border_slop, \n",
    "             title=title, normalizeLongs=normalize_lons, **options)\n",
    "    \n",
    "def ts_plot(ts_json, t_name='time', val_name='mean', \n",
    "            title='', units=''):\n",
    "    t = np.array([ts[0][t_name] for ts in ts_json[\"data\"]])\n",
    "    vals = np.array([ts[0][val_name] for ts in ts_json[\"data\"]])\n",
    "    show_plot(t, vals, title=textwrap.fill(title,64),\n",
    "              y_label=textwrap.fill(units,32))\n",
    "\n",
    "def plot_hovmoller(hm, time_key=\"time\", val_key=\"mean\",\n",
    "                   coord_series_key=\"lats\", coord_point_key=\"latitude\",\n",
    "                   coord_axis_vert=True, fill=-9999.,\n",
    "                   hovfig=None, subplot=111, add_color_bar=True,\n",
    "                   title=\"\"):\n",
    "    times = [d[time_key] for d in hm]\n",
    "    times = mdates.epoch2num(times)\n",
    "    coords = [[d[coord_point_key] for d in hvals[coord_series_key]]\n",
    "               for hvals in hm]\n",
    "    coords_flat = np.array(sorted(list(set(itertools.chain(*coords)))))\n",
    "    coords_delta = np.median(coords_flat[1:] - coords_flat[:-1])\n",
    "    coords_min = np.amin(coords_flat)\n",
    "    coords_max = np.amax(coords_flat)\n",
    "    vals_fill = np.full((len(hm),len(coords_flat)), fill, dtype=np.float64)\n",
    "    t_ind = 0\n",
    "    for hvals in hm:\n",
    "        cur_vals = np.array([d[val_key] for d in hvals[coord_series_key]])\n",
    "        coords = np.array([d[coord_point_key] for d in hvals[coord_series_key]])\n",
    "        coords_inds = np.round((coords - coords_min) /\n",
    "                               coords_delta).astype(int)\n",
    "        vals_fill[t_ind, coords_inds] = cur_vals\n",
    "        t_ind += 1\n",
    "    vals = np.ma.array(data=vals_fill, mask=vals_fill == fill)\n",
    "    extent = [np.min(times), np.max(times), coords_min, coords_max]\n",
    "    dtFmt = mdates.DateFormatter('%b %Y') # define the formatting\n",
    "    if hovfig is None:\n",
    "        fig = plt.figure(figsize=(16,6))\n",
    "    else:\n",
    "        fig = hovfig\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    ax.set_title(title)\n",
    "    if coord_axis_vert:\n",
    "        vals = np.transpose(vals)\n",
    "        ax.xaxis.set_major_formatter(dtFmt)\n",
    "        ax.set_ylabel(coord_point_key)\n",
    "        plt.xticks(rotation=45)\n",
    "    else:\n",
    "        extent = [extent[2], extent[3], extent[0], extent[1]]\n",
    "        ax.yaxis.set_major_formatter(dtFmt)\n",
    "        ax.set_xlabel(coord_point_key)\n",
    "    cax = ax.imshow(vals, origin='lower', extent=extent)\n",
    "    ax.set_aspect('auto')\n",
    "    if add_color_bar:\n",
    "        fig.colorbar(cax, ticks=np.linspace(np.min(vals), np.max(vals), 7),\n",
    "                     orientation='vertical')\n",
    "    return fig\n",
    "\n",
    "def compute_ts_and_tam_no_plot(dataset, bbox, start_time, end_time, base_url=\"localhost\",\n",
    "                               seasonal_filter=\"false\"):\n",
    "    url_params = 'ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}'.\\\n",
    "        format(dataset, *bbox.bounds, \n",
    "               start_time.strftime(dt_format), end_time.strftime(dt_format))\n",
    "    ts_url = '{}/timeSeriesSpark?{}&seasonalFilter={}'.format(base_url, url_params,\n",
    "                                                              seasonal_filter)\n",
    "    tam_url = '{}/timeAvgMapSpark?{}'.format(base_url, url_params)\n",
    "\n",
    "    # Display some information about the job\n",
    "    print(ts_url); print()\n",
    "    print(tam_url); print()\n",
    "\n",
    "    # Query SDAP to compute the time series\n",
    "    print(\"Computing time series...\")\n",
    "    start = time.perf_counter()\n",
    "    ts_json = requests.get(ts_url, verify=False).json()\n",
    "    print(\"Area-averaged time series took {} seconds\".format(time.perf_counter() - start))\n",
    "    print()\n",
    "    \n",
    "    # Query SDAP to compute the time averaged map\n",
    "    print(\"Computing time averaged map...\")\n",
    "    start = time.perf_counter()\n",
    "    tam_json = requests.get(tam_url, verify=False).json()\n",
    "    print(\"Time averaged map took {} seconds\".format(time.perf_counter() - start))\n",
    "    return ts_json, tam_json\n",
    "\n",
    "def compute_ts_and_tam(dataset, bbox, start_time, end_time, base_url=\"localhost\",\n",
    "                       seasonal_filter=\"false\", title='', grid_line_sep=5,\n",
    "                       units=None, log_colors=False, normalize_lons=False, **options):\n",
    "    ts_json, tam_json = compute_ts_and_tam_no_plot(dataset, bbox, start_time, end_time,\n",
    "                                                   base_url=base_url,\n",
    "                                                   seasonal_filter=seasonal_filter)\n",
    "    print()\n",
    "    print(\"Plot of area-average time series:\")\n",
    "    ts_plot(ts_json, val_name='mean', title=title, units=units)\n",
    "    if seasonal_filter == \"true\":\n",
    "        print(\"Plot of time series of difference with climatology:\")\n",
    "        ts_plot(ts_json, val_name='meanSeasonal',\n",
    "                title=title, units=units)\n",
    "        print()\n",
    "    \n",
    "    # Query SDAP to compute the time averaged map\n",
    "    tam = tam_json[\"data\"]\n",
    "    plot_map(tam, log_colors=log_colors, grid_line_sep=grid_line_sep, title=title, \n",
    "             normalize_lons=normalize_lons, **options)\n",
    "\n",
    "def show_sdap_json(j, nh=20, nt=10):\n",
    "    out_str = pformat(j)\n",
    "    for line in out_str.splitlines()[:nh]:\n",
    "        print(line)\n",
    "    print(\"\\t\\t.\\n\"*3)\n",
    "    for line in out_str.splitlines()[-nt:]:\n",
    "        print(line)\n",
    "\n",
    "print('Done with plotting setup.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Data Analytics Platform (SDAP)\n",
    "SDAP (https://sdap.apache.org/) provides advanced analytics capabilities to support NASA's New Observing Strategies (NOS) and Analytic Collaborative Frameworks (ACF) thrusts.  In this demonstration we use SDAP with oceanographic datasets relevant to the CEOS Ocean Variables Enabling Research and Applications for GEO (COVERAGE) initiative.\n",
    "\n",
    "In this demonstration, two geographically distributed SDAP cloud computing deployments are used, one on Amazon Web Services (AWS, https://aws.amazon.com/) for analytics with datasets curated in the USA (e.g., from NASA or NOAA), and one on WEkEO (https://www.wekeo.eu/) for analytics with European datasets (e.g., from CMEMS).  In this way we follow the strategy of performing the computations close to the data host providers.\n",
    "\n",
    "SDAP provides web service endpoints for each analytic algorithm, and can be accessed in a web browser or from a variety of programming languages.  This notebook demonstrates the Python API to access SDAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration Setup\n",
    "\n",
    "In the cell below, we specify the location of the SDAP deployments to use, a dataset to be used, the \n",
    "bounding box for an area of interest, and a time range for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base URLs for the USA (AWS) and European (WEkEO) SDAP deployments.\n",
    "base_url_us = \"https://coverage.ceos.org/nexus\"\n",
    "base_url_eu = \"https://coverage.wekeo.eu\"\n",
    "\n",
    "# Define bounding box and time period for analysis\n",
    "min_lon = -77; max_lon = -70\n",
    "min_lat = 35; max_lat = 42\n",
    "bbox = box(min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Specify the SDAP name of the datasets\n",
    "dataset_us = \"MUR25-JPL-L4-GLOB-v4.2_analysed_sst\"\n",
    "dataset_eu = \"METOFFICE-GLO-SST-L4-NRT-OBS-GMPE-V3_analysed_sst\"\n",
    "start_time = datetime(2018, 1, 1)\n",
    "end_time = datetime(2018, 12, 31)\n",
    "\n",
    "print(\"dataset_us: {}\".format(dataset_us))\n",
    "print(\"dataset_eu: {}\".format(dataset_eu))\n",
    "print(\"spatial region {}, and time range {} to {}.\".\n",
    "      format(bbox, start_time, end_time))\n",
    "plot_box(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Analytics\n",
    "## Data Inventory\n",
    "We begin by querying the SDAP `/list` endpoint at each of our SDAP deployments to examine what data are available in each instantiation of SDAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_sdap_inv(base_url):\n",
    "    url = '{}/list'.format(base_url)\n",
    "    print(\"Web Service Endpoint:\"); print(url);\n",
    "    res = requests.get(url, verify=False).json()\n",
    "    pprint(res)\n",
    "\n",
    "print(\"Response from AWS SDAP:\")\n",
    "get_sdap_inv(base_url_us)\n",
    "print()\n",
    "\n",
    "print(\"Response from WEkEO SDAP:\")\n",
    "get_sdap_inv(base_url_eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area-Averaged Time Series\n",
    "Next we will make a simple web service call to the SDAP `/timeSeriesSpark` endpoint.  This can also be done in a web browser or in a variety of programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute time series using the SDAP/NEXUS web/HTTP interface\n",
    "#\n",
    "# Construct the URL\n",
    "url = '{}/timeSeriesSpark?ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}&seasonalFilter={}'.\\\n",
    "    format(base_url_us, dataset_us, *bbox.bounds, \n",
    "           start_time.strftime(dt_format), end_time.strftime(dt_format),\n",
    "           \"false\")\n",
    "\n",
    "# Display some information about the job\n",
    "print(url); print()\n",
    "\n",
    "# Query SDAP to compute the time averaged map\n",
    "print(\"Waiting for response from SDAP...\")\n",
    "start = time.perf_counter()\n",
    "ts_json = requests.get(url, verify=False).json()\n",
    "print(\"Time series took {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON response\n",
    "The SDAP web service calls return the result in `JSON`, a standard web services data\n",
    "interchange format.  This makes it easy for another web service component to \"consume\" the SDAP output.\n",
    "Let's view the JSON response.  It is long, so we'll show just the first few time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_sdap_json(ts_json, nh=33, nt=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result\n",
    "Let's check our time series result with a plot. An SDAP dataset can also be associated with its climatology (long-term average for a given time period like monthly or daily). If this is the case, we can apply a \"seasonal filter\" to compute the spatial average of the difference between the dataset and its climatology as a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "print(\"Plot of area-average time series:\")\n",
    "ts_plot(ts_json, val_name='mean', title=dataset_us, units='Degrees Celsius')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Averaged Map\n",
    "Next we will issue an SDAP web service call to compute a time averaged map.  While the time series algorithm used above averages spatially to produce a single value for each time stamp, the time average map averages over time to produce a single value at each grid cell location.  While the time series produces a 1D result indexed by time, the time averaged map produces a 2D map indexed by latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute time-averaged map using the SDAP/NEXUS web/HTTP interface\n",
    "#\n",
    "# Construct the URL\n",
    "url = '{}/timeAvgMapSpark?ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}'.\\\n",
    "    format(base_url_us, dataset_us, *bbox.bounds, \n",
    "           start_time.strftime(dt_format), end_time.strftime(dt_format))\n",
    "\n",
    "# Display some information about the job\n",
    "print(url); print()\n",
    "\n",
    "# Query SDAP to compute the time averaged map\n",
    "print(\"Waiting for response from SDAP...\")\n",
    "start = time.perf_counter()\n",
    "tam_json = requests.get(url, verify=False).json()\n",
    "print(\"Time averaged map took {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON response\n",
    "The SDAP web service calls return the result in `JSON`, a standard web services data\n",
    "interchange format.  This makes it easy for another web service component to \"consume\" the SDAP output.\n",
    "Let's view the JSON response.  It is long, so we'll show just the first few grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_sdap_json(tam_json, nh=13, nt=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the actual data and plot the result\n",
    "The actual time averaged map data is readily accessible for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the actual output data\n",
    "tam = tam_json[\"data\"]\n",
    "\n",
    "# Create a plot of the Time Averaged Map results\n",
    "plot_map(tam, title=dataset_us+\" (deg C)\", grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hovmoller Maps\n",
    "Next we will issue an SDAP web service call to compute latitude-time and longitude-time Hovmoller maps and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the URLs\n",
    "url_lat = '{}/latitudeTimeHofMoellerSpark?ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}'.\\\n",
    "    format(base_url_us, dataset_us, *bbox.bounds, \n",
    "           start_time.strftime(dt_format), end_time.strftime(dt_format))\n",
    "url_lon = '{}/longitudeTimeHofMoellerSpark?ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}'.\\\n",
    "    format(base_url_us, dataset_us, *bbox.bounds, \n",
    "           start_time.strftime(dt_format), end_time.strftime(dt_format))\n",
    "\n",
    "# Query SDAP to compute the latitude-time Hovmoller map\n",
    "print(url_lat); print()\n",
    "print(\"Waiting for response from SDAP...\")\n",
    "start = time.perf_counter()\n",
    "hm_lat_json = requests.get(url_lat, verify=False).json()\n",
    "print(\"Latitude-time Hovmoller map took {} seconds\".format(time.perf_counter() - start)); print()\n",
    "\n",
    "# Query SDAP to compute the longitude-time Hovmoller map\n",
    "print(url_lon); print()\n",
    "print(\"Waiting for response from SDAP...\")\n",
    "start = time.perf_counter()\n",
    "hm_lon_json = requests.get(url_lon, verify=False).json()\n",
    "print(\"Longitude-time Hovmoller map took {} seconds\".format(time.perf_counter() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON response\n",
    "Let's view the JSON response. It is long, so we'll show just the first few grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show snippet of JSON response for latitude-time Hovmoller\n",
    "show_sdap_json(hm_lat_json, nh=19, nt=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show snippet of JSON response for longitude-time Hovmoller\n",
    "show_sdap_json(hm_lon_json, nh=19, nt=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the actual data and plot the results\n",
    "The actual map data is readily accessible for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the actual output data\n",
    "hm_lat = hm_lat_json[\"data\"]\n",
    "hm_lon = hm_lon_json[\"data\"]\n",
    "\n",
    "# Plot the Hovmoller maps\n",
    "hovfig = plot_hovmoller(hm_lat, coord_series_key=\"lats\", coord_point_key=\"latitude\", \n",
    "                        coord_axis_vert=True, subplot=121, \n",
    "                        title=\"Sea Surface Temperature (deg C)\")\n",
    "hovfig = plot_hovmoller(hm_lon, coord_series_key=\"lons\", coord_point_key=\"longitude\",\n",
    "                        coord_axis_vert=False, hovfig=hovfig, subplot=122,\n",
    "                        title=\"Sea Surface Temperature (deg C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Analytics Across AWS and WEkEO SDAP Deployments\n",
    "Next we can take advantage of the two SDAP deployments and conduct joint analytics across the two platforms.\n",
    "### Compare two SST datasets, one from AWS SDAP and one from WEkEO SDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous time series result was computed on AWS with\n",
    "# dataset \"MUR25-JPL-L4-GLOB-v4.2_analysed_sst\"\n",
    "ts_mur25_json = ts_json\n",
    "\n",
    "# Let's compute a 2nd SST time series, this time computed on WEkEO with\n",
    "# dataset \"METOFFICE-GLO-SST-L4-NRT-OBS-GMPE-V3_analysed_sst\"\n",
    "#\n",
    "dataset_eu_gmpe_sst = \"METOFFICE-GLO-SST-L4-NRT-OBS-GMPE-V3_analysed_sst\"\n",
    "url = '{}/timeSeriesSpark?ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}&seasonalFilter={}'.\\\n",
    "    format(base_url_eu, dataset_eu_gmpe_sst, *bbox.bounds, \n",
    "           start_time.strftime(dt_format), end_time.strftime(dt_format),\n",
    "           \"false\")\n",
    "\n",
    "# Display some information about the job\n",
    "print(url); print()\n",
    "\n",
    "# Query SDAP to compute the time averaged map\n",
    "print(\"Waiting for response from SDAP...\")\n",
    "start = time.perf_counter()\n",
    "ts_gmpe_json = requests.get(url, verify=False).json()\n",
    "print(\"Time series took {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "ts_plot_two(ts_mur25_json, ts_gmpe_json, dataset_us, dataset_eu_gmpe_sst, \n",
    "            \"Degrees Celsius\", \"Degrees Celsius\",\n",
    "            title=\"SST Comparison\", val_name=\"mean\")\n",
    "scatter_plot(ts_mur25_json, ts_gmpe_json, title=\"SST Comparison\",\n",
    "             xlabel=dataset_us, ylabel=dataset_eu_gmpe_sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SST from AWS SDAP and ADT from WEkEO SDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eu_cmems_adt = \"CMEMS_AVISO_SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047_adt\"\n",
    "url = '{}/timeSeriesSpark?ds={}&minLon={}&minLat={}&maxLon={}&maxLat={}&startTime={}&endTime={}&seasonalFilter={}'.\\\n",
    "    format(base_url_eu, dataset_eu_cmems_adt, *bbox.bounds, \n",
    "           start_time.strftime(dt_format), end_time.strftime(dt_format),\n",
    "           \"false\")\n",
    "\n",
    "# Display some information about the job\n",
    "print(url); print()\n",
    "\n",
    "# Query SDAP to compute the time averaged map\n",
    "print(\"Waiting for response from SDAP...\")\n",
    "start = time.perf_counter()\n",
    "ts_cmems_adt_json = requests.get(url, verify=False).json()\n",
    "print(\"Time series took {} seconds\".format(time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "ts_plot_two(ts_mur25_json, ts_cmems_adt_json, dataset_us, dataset_eu_cmems_adt, \n",
    "            \"Degrees Celsius\", \"Meters Above Geoid\",\n",
    "            title=\"SST vs ADT\", val_name=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More SDAP Analytics\n",
    "In the rest of this notebook we use a helper function defined in the first notebook cell above to use SDAP to compute time series and time averaged map for a variety of other relevant datasets.  In these results, SDAP is used in the same way as we demonstrated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Dynamic Topography (ADT) from CMEMS_AVISO_SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = \"CMEMS_AVISO_SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047_adt\"\n",
    "compute_ts_and_tam(dataset,\n",
    "                   bbox, start_time, end_time, base_url=base_url_eu, \n",
    "                   units=\"meters\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Level Anomaly (SLA) from CMEMS_AVISO_SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = \"CMEMS_AVISO_SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047_sla\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time, end_time, base_url=base_url_eu, \n",
    "                   units=\"meters\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Surface Salinity (SSS) from Multi-Mission Optimally Interpolated Sea Surface Salinity 7-Day Global Dataset V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_oisss7d = datetime(2011, 8, 28)\n",
    "end_time_oisss7d = datetime(2021, 9, 8)\n",
    "dataset = \"OISSS_L4_multimission_global_7d_v1.0_sss\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time_oisss7d, end_time_oisss7d, base_url=base_url_us, \n",
    "                   units=\"1e-3\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Surface Salinity (SSS) Uncertainty from Multi-Mission Optimally Interpolated Sea Surface Salinity 7-Day Global Dataset V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"OISSS_L4_multimission_global_7d_v1.0_sss_uncertainty\"\n",
    "start_time_oisss7d = datetime(2015, 7, 1)\n",
    "end_time_oisss7d = datetime(2021, 9, 8)\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time_oisss7d, end_time_oisss7d, base_url=base_url_us, \n",
    "                   units=\"1e-3\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Surface Salinity (SSS) from Multi-Mission Optimally Interpolated Sea Surface Salinity Monthly Global Dataset V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_oisssmo = datetime(2011, 9, 16)\n",
    "end_time_oisssmo = datetime(2021, 8, 16)\n",
    "dataset = \"OISSS_L4_multimission_global_monthly_v1.0_sss\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time_oisssmo, end_time_oisssmo, base_url=base_url_us, \n",
    "                   units=\"1e-3\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Surface Salinity (SSS) Anomaly from Multi-Mission Optimally Interpolated Sea Surface Salinity Monthly Global Dataset V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"OISSS_L4_multimission_global_monthly_v1.0_sss_anomaly\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time_oisssmo, end_time_oisssmo, base_url=base_url_us, \n",
    "                   units=\"1e-3\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Surface Temperature (SST) from MUR25-JPL-L4-GLOB-v4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = \"MUR25-JPL-L4-GLOB-v4.2_analysed_sst\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time, end_time, base_url=base_url_us, \n",
    "                   units=\"degrees celsius\", title=dataset, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chlorophyll-A from MODIS_Aqua_L3m_8D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define dataset and bounding box for analysis\n",
    "dataset = \"MODIS_Aqua_L3m_8D_chlor_a\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time, end_time, base_url=base_url_us,\n",
    "                   seasonal_filter=\"true\",\n",
    "                   units=\"milligram m-3\", title=dataset,\n",
    "                   log_colors=True, grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chlorophyll-A from JPL-MRVA25-CHL-L4-GLOB-v3.0_CHLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = \"JPL-MRVA25-CHL-L4-GLOB-v3.0_CHLA_analysis\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time, end_time, base_url=base_url_us, \n",
    "                   units=\"milligram m-3\", title=dataset, log_colors=True,\n",
    "                   grid_line_sep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chlorophyll-A from CMEMS_OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = \"CMEMS_OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082_CHL\"\n",
    "compute_ts_and_tam(dataset, \n",
    "                   bbox, start_time, end_time, base_url=base_url_eu, \n",
    "                   units=\"milligram m-3\", title=dataset, log_colors=True,\n",
    "                   grid_line_sep=2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
